{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pyreadr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "plt.rcParams['figure.figsize'] = [12, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [12, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/rds/general/project/hda-22-23/live/TDS/ek2018\") # set wd\n",
    "data = pyreadr.read_r('../ek2018/ML/final_data2.rds') # import data\n",
    "print(data.keys())\n",
    "data = data[None]\n",
    "\n",
    "# one hot encoding\n",
    "one_hot_encoded_cols = pd.get_dummies(data[['smoking_status', 'alcohol_drinker_status', \"employment_status\", \"qualifications\", \"chronotype\"]])\n",
    "\n",
    "# concatenate the one-hot encoded columns with the remaining columns in the original DataFrame\n",
    "data_encoded = pd.concat([data.drop(['smoking_status', 'alcohol_drinker_status', \"employment_status\", \"qualifications\", \"chronotype\"], axis=1), one_hot_encoded_cols], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert some cols to integers\n",
    "ordinal_cols = ['number_medications', 'days_walking', \"days_moderate_activity\", \"days_vigorous_activity\", \"time_tv\",\"time_phone\", \"sleep_duration\", \"alcohol_frequency\",\n",
    "               \"ethnicity\", \"time_outdoors_summer\", \"time_outdoors_winter\", \"time_computer\", \"avg_household_income\", \"urban_rural\", \"maternal_smoking_at_birth\", \"mother_illness\",\n",
    "               \"father_illness\"]\n",
    "# convert columns to integers\n",
    "for col in ordinal_cols:\n",
    "    data_encoded[col] = data_encoded[col].astype(int)\n",
    "    \n",
    "# move the outcome to be the last column and make it an integer\n",
    "last_column = data_encoded.pop('incident_case')  # remove column 'A' and store it in a variable\n",
    "data_encoded['incident_case'] = last_column\n",
    "\n",
    "data_encoded['incident_case'] = data_encoded['incident_case'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test split\n",
    "\n",
    "X = data_encoded.drop(columns=['incident_case'])\n",
    "Y = data_encoded['incident_case']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering: DBScan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_[:, 0], X_[:, 1])\n",
    "plt.title(\"Random Sample of Blobs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(234)\n",
    "# randomly sample 20,000 controls for computation\n",
    "# Select controls\n",
    "controls = data_encoded[data_encoded['incident_case'] == 0]\n",
    "\n",
    "# Sample 20,000 controls\n",
    "sampled_controls = controls.sample(n=20000, random_state=42)\n",
    "\n",
    "# Combine controls and cases\n",
    "sampled_data = pd.concat([sampled_controls, data_encoded[data_encoded['incident_case'] == 1]])\n",
    "\n",
    "# Shuffle the rows randomly\n",
    "sampled_data = sampled_data.sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sampled_data.shape)\n",
    "\n",
    "# remove outcome label\n",
    "clusterdata = sampled_data.drop(columns=['incident_case'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardise data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "clusterdata = scaler.fit_transform(clusterdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune epsilon \n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors=98).fit(clusterdata)\n",
    "distances, indices = nbrs.kneighbors(clusterdata)\n",
    "distanceDec = sorted(distances[:,9], reverse=True)\n",
    "plt.plot(range(1,len(clusterdata)+1), distanceDec)\n",
    "plt.xlabel('Points sorted by distance of 98th neighbor')\n",
    "plt.ylabel('Distance to 98th neighbor')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kneed import KneeLocator\n",
    "# compute sorted distances to the 10th nearest neighbor\n",
    "nbrs = NearestNeighbors(n_neighbors=98).fit(clusterdata)\n",
    "distances, indices = nbrs.kneighbors(clusterdata)\n",
    "distanceDec = sorted(distances[:,9], reverse=True)\n",
    "\n",
    "# create instance of KneeLocator\n",
    "kneedle = KneeLocator(range(1,len(clusterdata)+1), distanceDec, curve='convex', direction='decreasing')\n",
    "\n",
    "# retrieve the optimal epsilon value\n",
    "optimal_epsilon = kneedle.knee\n",
    "\n",
    "# print the optimal epsilon value\n",
    "print(f\"The optimal epsilon value is {optimal_epsilon}\")\n",
    "\n",
    "kneedle.plot_knee()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "range_n_clusters = [2, 3, 4]\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    cluster_labels = clusterer.fit_predict(clusterdata)\n",
    "    silhouette_avg = silhouette_score(clusterdata, cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "         \"the average silhouette score is\", silhouette_avg)\n",
    "    sample_silhouette_values = silhouette_samples(clusterdata, cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "db = DBSCAN(eps=8, min_samples=98, metric = 'manhattan')\n",
    "db.fit(clusterdata)\n",
    "#labels = db.labels_\n",
    "#fig = plt.figure(figsize=(10, 10))\n",
    "#sns.scatterplot(clusterdata[:,0], clusterdata[:,1], hue=[\"cluster-{}\".format(x) for x in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(db.labels_) # -1 means noisy points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster size\n",
    "from collections import Counter\n",
    "Counter(db.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.scatterplot(data = clusterdata, x = \"t-SNE-1\", y = \"t-SNE-2\", hue = db.labels_, legend = \"full\", palette = \"deep\")\n",
    "sns.move_legend(p, \"upper right\", bbox_to_anchor = (1.17, 1.), title = 'Clusters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tSNE you need to tune perplexity\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "perplexities = [5, 10, 20, 30, 50]\n",
    "for perplexity in perplexities:\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n",
    "    X_tsne = tsne.fit_transform(clusterdata)\n",
    "    plt.scatter(X_tsne[:,0], X_tsne[:,1])\n",
    "    plt.title(f\"Perplexity={perplexity}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "# Convert DataFrame to numpy array\n",
    "tsne_data = np.array(clusterdata)\n",
    "\n",
    "# Apply t-SNE to reduce the dimensionality of the data\n",
    "tsne = TSNE(n_components=2, perplexity=100, random_state=0, learning_rate = 368)\n",
    "X_tsne = tsne.fit_transform(tsne_data)\n",
    "\n",
    "\n",
    "# Define a colormap with 10 discrete colors (to handle arbitrary number of clusters)\n",
    "#n_clusters = len(set(dbscan.labels_)) - (1 if -1 in dbscan.labels_ else 0)\n",
    "#cmap = ListedColormap(['blue', 'green', 'red', 'cyan'][:n_clusters])\n",
    "\n",
    "# Plot the t-SNE embedding with discrete colors based on the DBSCAN cluster membership\n",
    "#plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=dbscan.labels_, cmap=cmap)\n",
    "#plt.colorbar()\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the transformed data with different colors for each cluster\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=db.labels_)\n",
    "plt.title(f\"t-SNE visualization with dbScan clustering\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering: KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA().fit(clusterdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# create PCA object\n",
    "pca = PCA()\n",
    "\n",
    "# fit PCA on data\n",
    "pca.fit(clusterdata)\n",
    "\n",
    "# get explained variance ratio\n",
    "explained_var_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# plot cumulative explained variance\n",
    "cumulative_var_ratio = np.cumsum(explained_var_ratio)\n",
    "plt.plot(cumulative_var_ratio)\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "plt.title('PCA Cumulative Explained Variance')\n",
    "plt.show()\n",
    "\n",
    "# find elbow point\n",
    "diffs = np.diff(cumulative_var_ratio)\n",
    "elbow = np.argmin(diffs) + 1\n",
    "\n",
    "# use elbow point as number of components\n",
    "print(f'Number of components at elbow point: {elbow}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=43)\n",
    "dementiaPCA = pca.fit_transform(clusterdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=2)\n",
    "dementiaPCA_labels = kmeans.fit_predict(dementiaPCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdementiaPCA = pd.DataFrame(dementiaPCA)\n",
    "dfdementiaPCA['cluster'] = dementiaPCA_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "X = dfdementiaPCA.iloc[:,:-1]\n",
    "Xtsne = TSNE(n_components=2, perplexity=30).fit_transform(X)\n",
    "dftsne = pd.DataFrame(Xtsne)\n",
    "dftsne['cluster'] = dementiaPCA_labels\n",
    "dftsne.columns = ['x1','x2','cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=dftsne,x='x1',y='x2',hue='cluster',legend=\"full\",alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "X = dfdementiaPCA.iloc[:,:-1]\n",
    "Xtsne = TSNE(n_components=2).fit_transform(X)\n",
    "dftsne = pd.DataFrame(Xtsne)\n",
    "dftsne['cluster'] = dementiaPCA_labels\n",
    "dftsne.columns = ['x1','x2','cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterdata['cluster'] = dementiaPCA_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterdata = pd.DataFrame(clusterdata) # have to do this for the code below to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some functions to plot just the variables that has significant deviation from global mean\n",
    "def outside_limit(df, label_col, label, sensitivity):\n",
    "  feature_list = clusterdata.columns[:-1]\n",
    "  \n",
    "  plot_list = []\n",
    "  mean_overall_list = []\n",
    "  mean_cluster_list = []\n",
    "  \n",
    "  for i,varname in enumerate(feature_list):\n",
    "    \n",
    "    #     get overall mean for a variable, set lower and upper limit\n",
    "    mean_overall = df[varname].mean()\n",
    "    lower_limit = mean_overall - (mean_overall*sensitivity)\n",
    "    upper_limit = mean_overall + (mean_overall*sensitivity)\n",
    "\n",
    "    #     get cluster mean for a variable\n",
    "    cluster_filter = df[label_col]==label\n",
    "    pd_cluster = df[cluster_filter]\n",
    "    mean_cluster = pd_cluster[varname].mean()\n",
    "    \n",
    "    #     create filter to display graph with 0.5 deviation from the mean\n",
    "    if mean_cluster <= lower_limit or mean_cluster >= upper_limit:\n",
    "      plot_list.append(varname)\n",
    "      mean_overall_std = mean_overall/mean_overall\n",
    "      mean_cluster_std = mean_cluster/mean_overall\n",
    "      mean_overall_list.append(mean_overall_std)\n",
    "      mean_cluster_list.append(mean_cluster_std)\n",
    "   \n",
    "  mean_df = pd.DataFrame({'feature_list':plot_list,\n",
    "                         'mean_overall_list':mean_overall_list,\n",
    "                         'mean_cluster_list':mean_cluster_list})\n",
    "  mean_df = mean_df.sort_values(by=['mean_cluster_list'], ascending=False)\n",
    "  \n",
    "  return mean_df\n",
    "\n",
    "def plot_barchart_all_unique_features(df, label_col, label, ax, sensitivity):\n",
    "  \n",
    "  mean_df = outside_limit(df, label_col, label, sensitivity)\n",
    "  mean_df_to_plot = mean_df.drop(['mean_overall_list'], axis=1)\n",
    "  \n",
    "  if len(mean_df.index) != 0:\n",
    "    sns.barplot(y='feature_list', x='mean_cluster_list', data=mean_df_to_plot, palette=sns.cubehelix_palette(20, start=.5, rot=-.75, reverse=True), \\\n",
    "                alpha=0.75, dodge=True, ax=ax)\n",
    "\n",
    "    for i,p in enumerate(ax.patches):\n",
    "      ax.annotate(\"{:.02f}\".format((p.get_width())), \n",
    "                  (1, p.get_y() + p.get_height() / 2.), xycoords=('axes fraction', 'data'),\n",
    "                  ha='right', va='top', fontsize=10, color='black', rotation=0, \n",
    "                  xytext=(0, 0),\n",
    "                  textcoords='offset pixels')\n",
    "  \n",
    "  ax.set_title('Unique Characteristics of Cluster ' + str(label))\n",
    "  ax.set_xlabel('Standardized Mean')\n",
    "  ax.axvline(x=1, color='k')\n",
    "\n",
    "def plot_features_all_cluster(df, label_col, n_clusters, sensitivity):\n",
    "  n_plot = n_clusters\n",
    "  fig, ax = plt.subplots(n_plot, 1, figsize=(12, n_plot*6), sharex='col')\n",
    "  ax= ax.ravel()\n",
    "  \n",
    "  label = np.arange(n_clusters)\n",
    "  for i in label:\n",
    "    plot_barchart_all_unique_features(df, label_col, label=i, ax=ax[i], sensitivity=sensitivity)\n",
    "    ax[i].xaxis.set_tick_params(labelbottom=True, rotation=45)\n",
    "    \n",
    "  plt.tight_layout()\n",
    "  display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features_all_cluster(df=clusterdata, label_col='cluster', n_clusters=2, sensitivity=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the centroid values for each cluster\n",
    "centroids = pca.inverse_transform(kmeans.cluster_centers_)\n",
    "\n",
    "# Convert centroid values back into original feature space\n",
    "cluster_features = pd.DataFrame(\n",
    "    data=centroids,\n",
    "    columns=clusterdata.columns[:-1]\n",
    ")\n",
    "\n",
    "# Print the feature values for each cluster\n",
    "for i, row in cluster_features.iterrows():\n",
    "    print(f\"Cluster {i}:\")\n",
    "    print(row.sort_values(ascending=False)[:5])\n",
    "    print()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterdata.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('over', SMOTE(sampling_strategy=0.4)),\n",
    "    ('under', RandomUnderSampler(sampling_strategy=0.6)),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "#param_grid = {\n",
    " #   'rf__n_estimators': [50, 100, 200],\n",
    "  #  'rf__max_depth': [2, 5, 10, 15]}\n",
    "\n",
    "param_grid = {\n",
    "    'rf__n_estimators': [10, 50, 100],\n",
    "    'rf__max_depth': [2, 5, 8],\n",
    "    'rf__min_samples_split': [2, 5, 10],\n",
    "    'rf__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Define the scoring metric for the grid search\n",
    "scoring = {'accuracy': 'accuracy', 'recall':'recall', 'F1':'f1'}\n",
    "\n",
    "# Create the grid search object\n",
    "grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, scoring=scoring,n_jobs=4, cv=5, return_train_score=True,refit='accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dimensions of the dataset before and after sampling\n",
    "print(f\"Original dataset shape: {X_train.shape}\")\n",
    "X_train_resampled, y_train_resampled = pipeline['over'].fit_resample(X_train, y_train)\n",
    "X_train_resampled, y_train_resampled = pipeline['under'].fit_resample(X_train_resampled, y_train_resampled)\n",
    "print('Resampled data shape:', X_train_resampled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data so i can use it in R\n",
    "X_train_resampled.to_csv('X_train_resampled.csv', index=False)\n",
    "y_train_resampled.to_csv('y_train_resampled.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nBalance of positive and negative classes (%):')\n",
    "y_train_resampled.value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best estimator from the grid search\n",
    "best_estimator = grid_search.best_estimator_\n",
    "print(best_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test data using the best estimator\n",
    "y_pred = best_estimator.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy score on the test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# Accuracy on the test set\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"\\033[1m Accuracy of Random forest on test set:\",\"{:.2%}\".format(accuracy_score(y_test, y_pred)))\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we should use average accuracy not true accuracy\n",
    "# Access the cv_results_ dictionary\n",
    "cv_results = grid_search.cv_results_\n",
    "\n",
    "# Get the mean test accuracy for each parameter combination\n",
    "mean_test_accuracy = cv_results['mean_test_accuracy']\n",
    "\n",
    "# Find the index of the best parameter combination\n",
    "best_index = np.argmax(mean_test_accuracy)\n",
    "\n",
    "# Get the best mean test accuracy\n",
    "best_accuracy = mean_test_accuracy[best_index]\n",
    "\n",
    "# Print the best accuracy\n",
    "print(\"Best accuracy:\", best_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balanced accuracy?\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "balanced_accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy on the training data\n",
    "y_train_pred = best_estimator.predict(X_train_resampled)\n",
    "accuracy_train = accuracy_score(y_train_resampled, y_train_pred)\n",
    "print(\"Train Accuracy:\", accuracy_train)\n",
    "\n",
    "print(classification_report(y_train_resampled, y_train_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the AUC score on the test data\n",
    "y_prob = best_estimator.predict_proba(X_test)[:, 1]\n",
    "auc_rf = roc_auc_score(y_test, y_prob)\n",
    "print(\"AUC:\", auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "# Print the classification matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix, test:\")\n",
    "print(cm)\n",
    "\n",
    "# Print the classification matrix\n",
    "cm_train = confusion_matrix(y_train_resampled, y_train_pred)\n",
    "print(\"Confusion Matrix, train:\")\n",
    "print(cm_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roc curve\n",
    "fpr_rf, tpr_rf, thresholds = roc_curve(y_test, y_prob)\n",
    "plt.plot(fpr_rf, tpr_rf, label='RF AUC = {:.2f}%'.format(auc*100))\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend(loc='lower right')\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tpr_rf)\n",
    "print(fpr_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "sns.heatmap(cm, annot=True, fmt='g',\n",
    "                       xticklabels=['Control', 'Dementia case'],\n",
    "            yticklabels=['Control', 'Dementia case'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "explainer = shap.TreeExplainer(best_estimator.named_steps['rf'])\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "plt.title('Random Forest SHAP plot for Dementia')\n",
    "shap.summary_plot(shap_values, features = X_test, class_inds = [1])\n",
    "plt.show()\n",
    "\n",
    "plt.title('Random Forest SHAP Density plot for Dementia')\n",
    "shap.summary_plot(shap_values[1], features = X_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the trained random forest classifier from the pipeline\n",
    "rf = grid_search.best_estimator_.named_steps['rf']\n",
    "# Get the feature importances\n",
    "importances = rf.feature_importances_\n",
    "indices = importances.argsort()\n",
    "features = X_train_resampled.columns[indices]\n",
    "\n",
    "# plot the feature importances as a horizontal bar chart\n",
    "plt.barh(range(len(indices)), importances[indices], align='center')\n",
    "plt.yticks(range(len(indices)), features)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Random Forest Feature Importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap plot\n",
    "import shap\n",
    "explainer = shap.TreeExplainer(rf_model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "plt.title('Random Forest SHAP plot for Dementia')\n",
    "shap.summary_plot(shap_values, features = X_test, class_inds = [1])\n",
    "plt.show()\n",
    "\n",
    "plt.title('Random Forest SHAP Density plot for Dementia')\n",
    "shap.summary_plot(shap_values[1], features = X_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap plot\n",
    "rf_model = grid_search.best_estimator_['rf']\n",
    "explainer = shap.TreeExplainer(rf_model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "plt.title('Random Forest SHAP plot for Dementia')\n",
    "shap.summary_plot(shap_values, features = X_test, class_inds = [1])\n",
    "plt.show()\n",
    "\n",
    "plt.title('Random Forest SHAP Density plot for Dementia')\n",
    "shap.summary_plot(shap_values[1], features = X_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test split NEW WITH LESS DATA. i didnt do this on RF because that one ran fine and i have no time left\n",
    "\n",
    "X2 = sampled_data.drop(columns=['incident_case'])\n",
    "Y2 = sampled_data['incident_case']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X2, Y2, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE \n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('over', SMOTE(sampling_strategy=0.4)),\n",
    "    ('under', RandomUnderSampler(sampling_strategy=0.6))\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "#pipeline.fit(X_train, y_train)\n",
    "\n",
    "#rint(f'''Shape of X before SMOTE: {X_train.shape}\n",
    "#Shape of X after SMOTE: {X_train.shape}''')\n",
    "\n",
    "#print('\\nBalance of positive and negative classes (%):')\n",
    "#Y_sm.value_counts(normalize=True) * 100\n",
    "\n",
    "# Check the dimensions of the dataset before and after sampling\n",
    "print(f\"Original dataset shape: {X_train.shape}\")\n",
    "X_resampled, y_resampled = pipeline['over'].fit_resample(X_train, y_train)\n",
    "X_resampled, y_resampled = pipeline['under'].fit_resample(X_resampled, y_resampled)\n",
    "print(f\"Resampled dataset shape: {X_resampled.shape}\")\n",
    "print(f\"Resampled dataset shape, Y: {y_resampled.shape}\")\n",
    "\n",
    "#print('\\nBalance of positive and negative classes (%):')\n",
    "y_resampled.value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('over', SMOTE(sampling_strategy=0.4)),\n",
    "    ('under', RandomUnderSampler(sampling_strategy=0.6)),\n",
    "    ('xgb', XGBClassifier(seed=42))\n",
    "])\n",
    "\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "#param_grid = {\n",
    " #   'rf__n_estimators': [50, 100, 200],\n",
    "  #  'rf__max_depth': [2, 5, 10, 15]}\n",
    "\n",
    "param_grid = {\n",
    "    'xgb__n_estimators': [50, 100, 200],\n",
    "    'xgb__max_depth': [5, 10, 15],\n",
    "    'xgb__min_child_weight': [1, 3, 5, 7],\n",
    "    'xgb__gamma':[i/10.0 for i in range(0,5)]\n",
    "}\n",
    "\n",
    "\n",
    "# Define the scoring metric for the grid search\n",
    "scoring = {'accuracy': 'accuracy', 'recall':'recall', 'F1':'f1'}\n",
    "\n",
    "# Create the grid search object\n",
    "grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, scoring=scoring,n_jobs=4, cv=5, return_train_score=True,refit='accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dimensions of the dataset before and after sampling\n",
    "print(f\"Original dataset shape: {X_train.shape}\")\n",
    "X_train_resampled, y_train_resampled = pipeline['over'].fit_resample(X_train, y_train)\n",
    "X_train_resampled, y_train_resampled = pipeline['under'].fit_resample(X_train_resampled, y_train_resampled)\n",
    "print('Resampled data shape:', X_train_resampled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best estimator from the grid search\n",
    "best_estimator = grid_search.best_estimator_\n",
    "print(best_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test data using the best estimator\n",
    "y_pred = best_estimator.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy on the test set\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"\\033[1m Accuracy of XGBoost on test set:\",\"{:.2%}\".format(accuracy_score(y_test, y_pred)))\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we should use average accuracy not true accuracy\n",
    "# Access the cv_results_ dictionary\n",
    "cv_results = grid_search.cv_results_\n",
    "\n",
    "# Get the mean test accuracy for each parameter combination\n",
    "mean_test_accuracy = cv_results['mean_test_accuracy']\n",
    "\n",
    "# Find the index of the best parameter combination\n",
    "best_index = np.argmax(mean_test_accuracy)\n",
    "\n",
    "# Get the best mean test accuracy\n",
    "best_accuracy = mean_test_accuracy[best_index]\n",
    "\n",
    "# Print the best accuracy\n",
    "print(\"Best accuracy:\", best_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balanced accuracy?\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "balanced_accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics on the training data\n",
    "y_train_pred = best_estimator.predict(X_train_resampled)\n",
    "accuracy_train = accuracy_score(y_train_resampled, y_train_pred)\n",
    "print(\"Train Accuracy:\", accuracy_train)\n",
    "\n",
    "print(classification_report(y_train_resampled, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the AUC score on the test data\n",
    "y_prob = best_estimator.predict_proba(X_test)[:, 1]\n",
    "auc = roc_auc_score(y_test, y_prob)\n",
    "print(\"AUC:\", auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "# roc curve\n",
    "fpr_xgb, tpr_xgb, thresholds = roc_curve(y_test, y_prob)\n",
    "plt.plot(fpr_xgb, tpr_xgb, label='XGBoost AUC = {:.2f}%'.format(auc*100))\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend(loc='lower right')\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tpr_xgb)\n",
    "print(fpr_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "sns.heatmap(cm, annot=True, fmt='g',\n",
    "                       xticklabels=['Control', 'Dementia case'],\n",
    "            yticklabels=['Control', 'Dementia case'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "# shap plot\n",
    "xgb_model = grid_search.best_estimator_['xgb']\n",
    "explainer = shap.Explainer(xgb_model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "#plt.title('XGBoost SHAP plot for Dementia')\n",
    "#shap.summary_plot(shap_values, features = X_test, class_inds = [1])\n",
    "#plt.show()\n",
    "\n",
    "# Plot the shap summary plot\n",
    "plt.title('XGBoost SHAP plot for Dementia')\n",
    "shap.summary_plot(shap_values, X_test, plot_type='bar', class_names=['No Dementia', 'Dementia'])\n",
    "plt.show()\n",
    "\n",
    "#plt.title('XGBoost SHAP Density plot for Dementia')\n",
    "#shap.summary_plot(shap_values[1], features = X_test)\n",
    "#plt.show()\n",
    "\n",
    "# Plot the shap dependence plots for each feature\n",
    "#for i in range(X_test.shape[1]):\n",
    " #   plt.title('XGBoost SHAP dependence plot for feature {}'.format(i))\n",
    "  #  shap.dependence_plot(i, shap_values, X_test)\n",
    "   # plt.show()\n",
    "\n",
    "shap_values = explainer(X_test)\n",
    "shap.summary_plot(shap_values, X_test, plot_type = \"violin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('over', SMOTE(sampling_strategy=0.4)),\n",
    "    ('under', RandomUnderSampler(sampling_strategy=0.6)),\n",
    "    ('standardscaler', StandardScaler()),\n",
    "    ('svm', SVC())\n",
    "])\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'svm__C': [0.1, 1, 10],\n",
    "    'svm__kernel': ['linear', 'rbf', 'sigmoid', 'poly'],\n",
    "    'svm__gamma': [1,0.1,0.01,0.001]\n",
    "}\n",
    "\n",
    "# Define the scoring metric for the grid search\n",
    "scoring = {'accuracy': 'accuracy', 'recall':'recall', 'F1':'f1'}\n",
    "\n",
    "# Create the grid search object\n",
    "grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, scoring=scoring,n_jobs=4, cv=5, return_train_score=True,refit='accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dimensions of the dataset before and after sampling\n",
    "print(f\"Original dataset shape: {X_train.shape}\")\n",
    "X_train_resampled, y_train_resampled = pipeline['over'].fit_resample(X_train, y_train)\n",
    "X_train_resampled, y_train_resampled = pipeline['under'].fit_resample(X_train_resampled, y_train_resampled)\n",
    "X_train_resampled = pipeline['standardscaler'].fit_transform(X_train_resampled)\n",
    "\n",
    "print('Resampled data shape:', X_train_resampled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best estimator from the grid search\n",
    "best_estimator = grid_search.best_estimator_\n",
    "print(best_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test data using the best estimator\n",
    "y_pred = best_estimator.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy score on the test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# Accuracy on the test set\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"\\033[1m Accuracy of Random forest on test set:\",\"{:.2%}\".format(accuracy_score(y_test, y_pred)))\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we should use average accuracy not true accuracy\n",
    "# Access the cv_results_ dictionary\n",
    "cv_results = grid_search.cv_results_\n",
    "\n",
    "# Get the mean test accuracy for each parameter combination\n",
    "mean_test_accuracy = cv_results['mean_test_accuracy']\n",
    "\n",
    "# Find the index of the best parameter combination\n",
    "best_index = np.argmax(mean_test_accuracy)\n",
    "\n",
    "# Get the best mean test accuracy\n",
    "best_accuracy = mean_test_accuracy[best_index]\n",
    "\n",
    "# Print the best accuracy\n",
    "print(\"Best accuracy:\", best_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy on the training data\n",
    "y_train_pred = best_estimator.predict(X_train_resampled)\n",
    "accuracy_train = accuracy_score(y_train_resampled, y_train_pred)\n",
    "print(\"Train Accuracy:\", accuracy_train)\n",
    "\n",
    "print(classification_report(y_train_resampled, y_train_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the AUC score on the test data\n",
    "y_prob = best_estimator.predict_proba(X_test)[:, 1]\n",
    "auc = roc_auc_score(y_test, y_prob)\n",
    "print(\"AUC:\", auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the predicted scores on the test data\n",
    "y_scores = best_estimator.decision_function(X_test)\n",
    "\n",
    "# Calculate the AUC score on the test data\n",
    "auc = roc_auc_score(y_test, y_scores)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the false positive rate and true positive rate\n",
    "fpr_svm, tpr_svm, thresholds= roc_curve(y_test, y_scores)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.plot(fpr_svm, tpr_svm, lw=2, label='SVM AUC = {:.2f}%'.format(auc*100))\n",
    "plt.plot([0, 1], [0, 1],lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "sns.heatmap(cm, annot=True, fmt='g',\n",
    "                       xticklabels=['Control', 'Dementia case'],\n",
    "            yticklabels=['Control', 'Dementia case'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap plot\n",
    "svm_model = grid_search.best_estimator_['svm']\n",
    "explainer = shap.Explainer(svm_model, X_train_resampled)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "plt.title('SVM SHAP plot for Dementia')\n",
    "shap.summary_plot(shap_values, features = X_test, class_inds = [1])\n",
    "plt.show()\n",
    "\n",
    "plt.title('SVM SHAP Density plot for Dementia')\n",
    "shap.summary_plot(shap_values[1], features = X_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate the summary plot\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\", feature_names=X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot all AUCs together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot SVM ROC curve\n",
    "plt.plot(fpr_svm, tpr_svm, label='SVM')\n",
    "\n",
    "# Plot XGBoost ROC curve\n",
    "plt.plot(fpr_xgb, tpr_xgb, label='XGBoost', linewidth=5)\n",
    "\n",
    "# Plot Random Forest ROC curve\n",
    "plt.plot(fpr_rf, tpr_rf, label='Random Forest')\n",
    "\n",
    "# Set axis labels and title\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "plt.plot([0, 1], [0, 1],lw=2, linestyle='--')\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
